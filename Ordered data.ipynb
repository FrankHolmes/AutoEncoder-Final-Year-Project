{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e7d673-45cd-4bad-b3ab-c2f207f61489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil, log, exp\n",
    "from sklearn import preprocessing\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "tf.keras.utils.set_random_seed(seed=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce1a3b-1eba-4988-8038-2107db0cc4db",
   "metadata": {},
   "source": [
    "## AutoEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ea7ac3a-4f85-448f-a6ed-fcf82b1d1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(tf.keras.Model):\n",
    "    def __init__(self, x_train):\n",
    "        super().__init__()\n",
    "        self.encoder = self.encoder_model()\n",
    "        self.decoder = self.decoder_model()\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.train_data = x_train\n",
    "        \n",
    "\n",
    "    def encoder_model(self):\n",
    "        encoder_inputs = keras.Input(shape=(6,)) # the shape is the number of dimensions and the length of data that will be put in. \n",
    "        # i.e. 6dims and 1 data points\n",
    "        \n",
    "        \n",
    "\n",
    "        encoded = layers.Dense(5, activation='tanh')(encoder_inputs)\n",
    "        encoded = layers.Dense(4, activation='tanh')(encoded)\n",
    "        latent_space = layers.Dense(3, activation='tanh')(encoded) # this is the latent space (the output of the encoder)\n",
    "        \n",
    "        encoder = keras.Model(encoder_inputs, latent_space) # this defines the model in the keras computational graph\n",
    "        encoder.compile(optimizer='Adam', loss='mse') # the compile step of the keras model\n",
    "        encoder.summary()\n",
    "        return encoder # returns the compiled model\n",
    "    \n",
    "    def decoder_model(self):\n",
    "        decoder_inputs = keras.Input(shape=(3,)) # these are the inputs of the decoder (3dims (the latent space), 1 number of data points)\n",
    "        decoded = layers.Dense(4, activation='tanh')(decoder_inputs)\n",
    "        decoded = layers.Dense(5, activation='tanh')(decoded)     \n",
    "        output_space = layers.Dense(6, activation='sigmoid')(decoded) # the output (decoded latent space, should be the inputs)\n",
    "        \n",
    "        decoder = keras.Model(decoder_inputs, output_space) # defining the decoder as a Model\n",
    "        decoder.compile(optimizer='Adam', loss='mse') # compiling the model\n",
    "        decoder.summary()\n",
    "        return decoder # returning the compiled model\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_step(self, data): # as we have two parts of the model, we have to define how the model should be trained to access the latent space\n",
    "        with tf.GradientTape() as tape:\n",
    "            latent_output = self.encoder(data)\n",
    "            reconstructed_data = self.decoder(latent_output)\n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.mean_squared_logarithmic_error(data, reconstructed_data)))\n",
    "            total_loss = reconstruction_loss\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        return {'total_loss': self.total_loss_tracker.result(),\n",
    "                'reconstruction_loss': self.reconstruction_loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9917b269-a31d-4d92-8e77-4a0bb3ed42a4",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365416dc-3902-4a4e-97f1-a1a4bf2ff5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "\n",
    "    def map_unique_values(self, arr): #function to enforce even spacing in categorical data\n",
    "        unique_values = list(set(arr))\n",
    "        unique_values.sort() \n",
    "        value_to_number_mapping = {value: i + 1 for i, value in enumerate(unique_values)}\n",
    "        mapped_array = np.array([value_to_number_mapping[value] for value in arr])\n",
    "        return mapped_array\n",
    "    \n",
    "    def map_unique_values_to_columns(self, np_array):\n",
    "        mapped_columns = []\n",
    "        for col_idx in range(np_array.shape[1]):\n",
    "            mapped_column = self.map_unique_values(np_array[:, col_idx])\n",
    "            mapped_columns.append(mapped_column)\n",
    "        return np.array(mapped_columns).T\n",
    "\n",
    "    \n",
    "\n",
    "    def order_by_frequency_array(self, data): #function used to value each data point based on freq\n",
    "        value_counts = {}\n",
    "        for value in data:\n",
    "            if value in value_counts:\n",
    "                value_counts[value] += 1\n",
    "            else:\n",
    "                value_counts[value] = 1\n",
    "                \n",
    "        sorted_values = sorted(value_counts.keys(), key=lambda x: value_counts[x], reverse=False)\n",
    "        rank_mapping = {value: rank for rank, value in enumerate(sorted_values, start=1)}\n",
    "        ranked_data = [rank_mapping[value] for value in data]\n",
    "    \n",
    "        return ranked_data\n",
    "\n",
    "\n",
    "    def order_by_frequency(self, array): #function used to carry out valueing over each column\n",
    "        output = []\n",
    "        for column in array.T:\n",
    "            ranked_data = self.order_by_frequency_array(column)\n",
    "            output.append(ranked_data)\n",
    "        output_array = np.column_stack(output)\n",
    "    \n",
    "        return output_array\n",
    "\n",
    "    \n",
    "    \n",
    "    def min_max(array):\n",
    "        min_val = np.min(array, axis=0)\n",
    "        max_val = np.max(array, axis=0)\n",
    "        normalized_array = (array - min_val) / (max_val - min_val)\n",
    "        return normalized_array\n",
    "\n",
    "\n",
    "    def min_max_odd_columns(array):\n",
    "        normalized_array = np.empty_like(array)\n",
    "        for i in range(array.shape[1]):\n",
    "            if i % 2 != 0:\n",
    "                normalized_array[:, i] = array[:, i]\n",
    "            else: \n",
    "                min_val = np.min(array[:, i])\n",
    "                max_val = np.max(array[:, i])\n",
    "                normalized_array[:, i] = (array[:, i] - min_val) / (max_val - min_val)\n",
    "        return normalized_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947343dd-0c15-4894-a236-16111c01415f",
   "metadata": {},
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb221dd9-6008-4cea-abc8-2bd40cabfac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class clustering:\n",
    "\n",
    "    def assign_clusters_find_distances(self, new_data, centroids):\n",
    "            # calculate distances to centroids\n",
    "            distances = np.linalg.norm(new_data[:, np.newaxis] - centroids, axis=2)\n",
    "            \n",
    "            # Assign each point to the closest centroid\n",
    "            cluster_assignment = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Get distances to the closest centroid\n",
    "            min_distances = np.min(distances, axis=1)\n",
    "            \n",
    "            return cluster_assignment, min_distances\n",
    "\n",
    "\n",
    "    def kmean_clustering_with_distances(self, data, num_clusters):\n",
    "        num_columns = data.shape[1]\n",
    "        centroids_list = []\n",
    "        assignment_distances_list = []\n",
    "        \n",
    "        # Iterate over each column\n",
    "        for i in range(num_columns):\n",
    "            # remove i-th column from the data\n",
    "            data_subset = np.delete(data, i, axis=1)\n",
    "            \n",
    "            #K-means clustering\n",
    "            kmeans = kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=10).fit(data_subset)\n",
    "            \n",
    "            # record cluster centroids\n",
    "            centroids = kmeans.cluster_centers_\n",
    "            centroids_list.append(centroids)\n",
    "            \n",
    "            # get clusters and distances\n",
    "            cluster_assignment, distances = self.assign_clusters_find_distances(data_subset, centroids)\n",
    "            assignment_distances_list.append(cluster_assignment)\n",
    "            assignment_distances_list.append(distances)\n",
    "        \n",
    "        # stack centroids into a single array\n",
    "        stacked_centroids = np.stack(centroids_list, axis=1)\n",
    "    \n",
    "        # stack assignments and distances into a single array\n",
    "        stacked_assignment_distances = np.column_stack(assignment_distances_list)\n",
    "        \n",
    "        return stacked_centroids, stacked_assignment_distances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def kmeans_clustering_with_distances_new_data(self, stacked_centroids, data): \n",
    "            num_columns = stacked_centroids.shape[1]\n",
    "            assignment_distances_list = []\n",
    "            \n",
    "            # Iterate over each column\n",
    "            for i in range(num_columns):\n",
    "                # Get the centroids for the current iteration\n",
    "                centroids = stacked_centroids[:, i, :]\n",
    "                \n",
    "                # Remove the i-th column from the data\n",
    "                data_subset = np.delete(data, i, axis=1)\n",
    "                \n",
    "                # Assign clusters and distances using the provided centroids\n",
    "                cluster_assignment, distances = self.assign_clusters_find_distances(data_subset, centroids)\n",
    "                assignment_distances_list.append(cluster_assignment)\n",
    "                assignment_distances_list.append(distances)\n",
    "            \n",
    "            # Stack assignments and distances into a single array\n",
    "            assignment_distances = np.column_stack(assignment_distances_list)\n",
    "            \n",
    "            return assignment_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f62f08-a09f-4ac3-8128-de7637337d85",
   "metadata": {},
   "source": [
    "## Data Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c86106d7-316f-44b6-8011-c15207acefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_analysis:\n",
    "\n",
    "    def calculate_gradient(self, x, y):\n",
    "        if len(x) != len(y):\n",
    "            raise ValueError(\"Arrays must be of the same length\")\n",
    "        \n",
    "        # Perform linear regression to find the slope (gradient)\n",
    "        m, _ = np.polyfit(x, y, 1)\n",
    "        \n",
    "        return m\n",
    "\n",
    "    def calculate_average_grad(self, data_set1, data_set2):\n",
    "        gradients = []\n",
    "        for i in range(data_set1.shape[1]):\n",
    "            gradient = self.calculate_gradient(data_set1[:, i], data_set2[:, i])\n",
    "            gradients.append(gradient)\n",
    "        \n",
    "        gradients = np.array(gradients)\n",
    "        average_gradient = np.mean(gradients)\n",
    "        \n",
    "        return gradients, average_gradient\n",
    "\n",
    "\n",
    "    def calculate_r_square(self, array1, array2):\n",
    "            if array1.shape[1] != array2.shape[1]:\n",
    "                raise ValueError(\"Arrays must have the same number of columns\")\n",
    "            \n",
    "            r_squared_values = []\n",
    "            for i in range(array1.shape[1]):\n",
    "                correlation_matrix = np.corrcoef(array1[:, i], array2[:, i])\n",
    "                correlation_coefficient = correlation_matrix[0, 1]\n",
    "                r_squared = correlation_coefficient ** 2\n",
    "                r_squared_values.append(r_squared)\n",
    "        \n",
    "            r_squared_values = np.array(r_squared_values)\n",
    "            average_r_squared = np.mean(r_squared_values)\n",
    "            \n",
    "            return r_squared_values, average_r_squared\n",
    "    \n",
    "    def plot_hexbin_log_frequency(x, y, xlabel=\"\", ylabel=\"\", title=\"\", gridsize=100, cmap='viridis', colorbar=True):\n",
    "        \"\"\"\n",
    "        Function to create a hexbin plot with log-scaled frequency using Matplotlib.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        # Calculate the hexbin frequencies\n",
    "        hb = plt.hexbin(x, y, gridsize=gridsize, cmap=cmap)\n",
    "        counts = hb.get_array()\n",
    "        nonzero_counts = counts[counts != 0]\n",
    "        \n",
    "        # Set the norm to LogNorm with vmin=0\n",
    "        norm = mcolors.LogNorm(vmin=1, vmax=nonzero_counts.max())\n",
    "        hb = plt.hexbin(x, y, gridsize=gridsize, cmap=cmap, norm=norm)\n",
    "        \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        \n",
    "        if colorbar:\n",
    "            plt.colorbar(hb, label='log(count)')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e2f2cc-77c4-453f-83dd-8d63f3a97c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_raw = np.loadtxt('train4 .csv',delimiter = ',') #import training data as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7df7559c-9023-4e67-b1bf-267d79bcff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate categorical and continuous\n",
    "categorical_data = np.delete(x_data_raw, 5, axis=1)\n",
    "x6 = x_data_raw[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bc8a37c-84c0-4765-b725-da00fe7ca981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instances for classes\n",
    "preprocessor = preprocessing()\n",
    "Cluster = clustering()\n",
    "Analyse = data_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdb3aaf3-4b2c-43d7-983d-f8831b968c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.72889694 1.         0.98461538 0.98848624 0.97415385]\n",
      " [0.01786639 1.         1.         0.883091   0.88      ]\n",
      " [0.17115484 1.         0.96923077 0.99785962 0.99938462]\n",
      " ...\n",
      " [0.88114966 1.         0.93846154 0.75798952 0.79630769]\n",
      " [0.17400311 1.         0.93846154 0.87881024 0.99938462]\n",
      " [0.42180218 1.         0.96923077 0.9988191  0.99815385]]\n"
     ]
    }
   ],
   "source": [
    "#Preprocess Categorical Data\n",
    "\n",
    "#apply even spacing\n",
    "\n",
    "mapped = preprocessor.map_unique_values_to_columns(categorical_data)\n",
    "\n",
    "#order\n",
    "\n",
    "ordered = preprocessor.order_by_frequency(mapped)\n",
    "\n",
    "\n",
    "#normalise \n",
    "ordered_min_max = preprocessing.min_max(ordered)\n",
    "\n",
    "print(ordered_min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "765d070d-72f5-4034-ae70-f12231ec5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Continuous Data\n",
    "\n",
    "log_x6 = np.log10(x6)\n",
    "x6_logged_normalized = preprocessing.min_max(log_x6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddc5dd23-d58f-4911-858c-22fa658964a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_data = np.column_stack((ordered_min_max, x6_logged_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "558464a1-e34c-418c-a312-efaf75688a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Input_data\n",
    "#x_test = Input_data[ceil(0.8*len(Input_data)):] change this to validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863bfe25-3fcb-4188-a18c-c8ddadfcf5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train AutoEncoder #after changing all test stuff, check that auto encoder still runs without test bit\n",
    "\n",
    "AutoEncoder = AutoEncoder(x_train)\n",
    "AutoEncoder.compile(optimizer=keras.optimizers.Adam())\n",
    "AutoEncoder.fit(x_train, epochs= 2000 , batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c051c0-740a-4a9a-b844-6ce6eb68ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run with Trianing Data\n",
    "\n",
    "latent_space_representation = AutoEncoder.encoder.predict(Input_data)\n",
    "\n",
    "Output_data = AutoEncoder.decoder.predict(latent_space_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a1cf4-c8ec-4a16-8839-34c813b55fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Reconstruction Loss\n",
    "Reconstruction_loss = np.mean(np.square(x_train - Output_data), axis = 1)\n",
    "Threshold = np.percentile(Reconstruction_loss, 99.0)\n",
    "print(Threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf0b52-1c77-4496-98b3-45a204b2b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Gradients\n",
    "all_gradients, average_gradient = Analyse.calculate_average_grad(Input_data, Output_data)\n",
    "print(average_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67244e-0afe-49e2-9237-8dd44f512595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find r^2 values\n",
    "all_r_squared, average_r_squared = Analyse.calculate_r_square(Input_data, Output_data)\n",
    "print(average_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa1978-373a-40df-94da-f10f43ea02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot input vs output\n",
    "#run through this to check it works with reset kernal!\n",
    "data_analysis.plot_hexbin_log_frequency(Input_data[:,0], Output_data[:, 0] , xlabel='Input', ylabel='Output', title='') #change index to access different variable plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c31e414-311b-4a7b-81c3-38b64a9f14dd",
   "metadata": {},
   "source": [
    "## Test Trained Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827f609-7afd-4693-a8bf-69c95b2b6568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import test data as numpy array \n",
    "Testing_data = np.loadtxt('testing_data2.csv',delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48851961-4997-46d1-ba96-d58e47ddaee8",
   "metadata": {},
   "source": [
    "## Preprocess Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833915dd-87ab-4b80-889b-26d49f2deb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate categorical and continuous\n",
    "\n",
    "categorical_data_test = np.delete(Testing_data, 5, axis=1)\n",
    "x6_test = Testing_data[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5521f6-5f7e-4edd-b178-83981d4924fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Categorical Data\n",
    "\n",
    "#apply even spacing\n",
    "mapped_test = preprocessor.map_unique_values_to_columns(categorical_data_test)\n",
    "\n",
    "\n",
    "#order\n",
    "\n",
    "ordered_test = preprocessor.order_by_frequency(mapped_test)\n",
    "\n",
    "#normalise \n",
    "ordered_min_max_test = preprocessing.min_max(ordered_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c89397-47d6-4fc2-81d4-3f3860c5dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess continuous data\n",
    "\n",
    "log_x6_test = np.log10(x6_test)\n",
    "x6_logged_normalized_test = preprocessing.min_max(log_x6_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06568c32-40c4-4c8e-a5c5-df40d5fb2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_data_test = np.column_stack((ordered_min_max_test, x6_logged_normalized_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f53259-aa72-4d1b-9063-4c0134d70480",
   "metadata": {},
   "source": [
    "## AutoEncoder on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d72def-bf32-4371-b9eb-4ad72674f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_representation_test = AutoEncoder.encoder.predict(Input_data_test)\n",
    "\n",
    "Output_data_test = AutoEncoder.decoder.predict(latent_space_representation_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd285a5-4de2-4b90-b6e4-6a19e2341fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reconstruction_loss = np.mean(np.square(Input_data_test - Output_data_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d6bdf-c263-4701-963b-d325c368aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_truth = Reconstruction_loss > Threshold\n",
    "\n",
    "anomaly_indices = np.where(anomaly_truth)[0]\n",
    "\n",
    "anomalies = Input_data_test[anomaly_truth]\n",
    "\n",
    "print(\"Anomalies:\")\n",
    "print(anomalies)\n",
    "\n",
    "print(\"Indices of anomalies in Test_data:\")\n",
    "print(anomaly_indices)\n",
    "\n",
    "print(\"Number of anomalies:\", len(anomalies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd1bfc-b592-4382-aecb-584fa74b9b2f",
   "metadata": {},
   "source": [
    "## Test Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f9487-fe22-4293-9f6c-5589690672e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_grad, Test_grad_average = Analyse.calculate_average_grad(Input_data_test, Output_data_test)\n",
    "print(Test_grad_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe74bb7-18a6-4d34-b118-4658ebe70458",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_R2, Test_R2_average = Analyse.calculate_r_square(Input_data_test, Output_data_test)\n",
    "print(Test_R2_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8147844-1a41-4b76-9029-530d32fcfc16",
   "metadata": {},
   "source": [
    "## Compare to George IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df437a4-1745-4693-8928-08bcb4d2ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('George_Anomalies.csv')\n",
    "George_IDs = df['id']\n",
    "array_of_values = np.array(George_IDs, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a106b9-30bf-4264-8137-bcfd1cdbdc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the intersection of the two arrays\n",
    "intersection = np.intersect1d(George_IDs, anomaly_indices)\n",
    "\n",
    "# Get the number of values shared\n",
    "shared_count = len(intersection)\n",
    "\n",
    "print(\"Number of shared values:\", shared_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c02a39-c415-4ab3-a613-36112d6b2fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc250613-ca33-4796-a102-f8b55575f5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
