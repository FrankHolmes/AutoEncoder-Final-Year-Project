{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5c382-630e-4407-a686-b5cf388e26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil, log, exp\n",
    "from sklearn import preprocessing\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "tf.keras.utils.set_random_seed(seed=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ec4dc-0f12-4b84-afac-4d30e4195da5",
   "metadata": {},
   "source": [
    "## AutoEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170dab0b-2db2-44d2-b088-20bda6e82ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(tf.keras.Model):\n",
    "    def __init__(self, x_train):\n",
    "        super().__init__()\n",
    "        self.encoder = self.encoder_model()\n",
    "        self.decoder = self.decoder_model()\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.train_data = x_train\n",
    "        \n",
    "\n",
    "    def encoder_model(self):\n",
    "        encoder_inputs = keras.Input(shape=(11,)) # the shape is the number of dimensions and the length of data that will be put in. \n",
    "        # i.e. 6dims and 1 data points\n",
    "        \n",
    "        \n",
    "\n",
    "        encoded = layers.Dense(9, activation='tanh')(encoder_inputs)\n",
    "        encoded = layers.Dense(7, activation='tanh')(encoded)\n",
    "        encoded = layers.Dense(5, activation='tanh')(encoded)\n",
    "        latent_space = layers.Dense(3, activation='tanh')(encoded) # this is the latent space (the output of the encoder)\n",
    "        \n",
    "        encoder = keras.Model(encoder_inputs, latent_space) # this defines the model in the keras computational graph\n",
    "        encoder.compile(optimizer='Adam', loss='mse') # the compile step of the keras model\n",
    "        encoder.summary()\n",
    "        return encoder # returns the compiled model\n",
    "    \n",
    "    def decoder_model(self):\n",
    "        decoder_inputs = keras.Input(shape=(3,)) # these are the inputs of the decoder (3dims (the latent space), 1 number of data points)\n",
    "        decoded = layers.Dense(5, activation='tanh')(decoder_inputs)\n",
    "        decoded = layers.Dense(7, activation='tanh')(decoded)\n",
    "        decoded = layers.Dense(9, activation='tanh')(decoded)     \n",
    "        output_space = layers.Dense(11, activation='sigmoid')(decoded) # the output (decoded latent space, should be the inputs)\n",
    "        \n",
    "        decoder = keras.Model(decoder_inputs, output_space) # defining the decoder as a Model\n",
    "        decoder.compile(optimizer='Adam', loss='mse') # compiling the model\n",
    "        decoder.summary()\n",
    "        return decoder # returning the compiled model\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_step(self, data): # as we have two parts of the model, we have to define how the model should be trained to access the latent space\n",
    "        with tf.GradientTape() as tape:\n",
    "            latent_output = self.encoder(data)\n",
    "            reconstructed_data = self.decoder(latent_output)\n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.mean_squared_logarithmic_error(data, reconstructed_data)))\n",
    "            total_loss = reconstruction_loss\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        return {'total_loss': self.total_loss_tracker.result(),\n",
    "                'reconstruction_loss': self.reconstruction_loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f748c-bfb5-4b88-8d78-bf0c23e7c7d6",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef21d51-6f01-4262-b2a9-8e5a45cfce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "\n",
    "    def map_unique_values(self, arr):\n",
    "        unique_values = list(set(arr))\n",
    "        unique_values.sort() \n",
    "        value_to_number_mapping = {value: i + 1 for i, value in enumerate(unique_values)}\n",
    "        mapped_array = np.array([value_to_number_mapping[value] for value in arr])\n",
    "        return mapped_array\n",
    "    \n",
    "    def map_unique_values_to_columns(self, np_array):\n",
    "        mapped_columns = []\n",
    "        for col_idx in range(np_array.shape[1]):\n",
    "            mapped_column = self.map_unique_values(np_array[:, col_idx])\n",
    "            mapped_columns.append(mapped_column)\n",
    "        return np.array(mapped_columns).T\n",
    "    \n",
    "    \n",
    "    def min_max(array):\n",
    "        min_val = np.min(array, axis=0)\n",
    "        max_val = np.max(array, axis=0)\n",
    "        normalized_array = (array - min_val) / (max_val - min_val)\n",
    "        return normalized_array\n",
    "\n",
    "\n",
    "    def min_max_odd_columns(array):\n",
    "        normalized_array = np.empty_like(array)\n",
    "        for i in range(array.shape[1]):\n",
    "            if i % 2 != 0:  # Even columns, just copy\n",
    "                normalized_array[:, i] = array[:, i]\n",
    "            else:  # Odd columns, perform min-max normalization\n",
    "                min_val = np.min(array[:, i])\n",
    "                max_val = np.max(array[:, i])\n",
    "                normalized_array[:, i] = (array[:, i] - min_val) / (max_val - min_val)\n",
    "        return normalized_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16555805-2c58-4531-879f-bec21ce23df6",
   "metadata": {},
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ed649-a8e4-4881-b9fc-00fdb62fdad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class clustering:\n",
    "\n",
    "    def assign_clusters_find_distances(self, new_data, centroids):\n",
    "            # Calculate distances to centroids\n",
    "            distances = np.linalg.norm(new_data[:, np.newaxis] - centroids, axis=2)\n",
    "            \n",
    "            # Assign each point to the closest centroid\n",
    "            cluster_assignment = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Get distances to the closest centroid\n",
    "            min_distances = np.min(distances, axis=1)\n",
    "            \n",
    "            return cluster_assignment, min_distances\n",
    "\n",
    "\n",
    "    def kmean_clustering_with_distances(self, data, num_clusters):\n",
    "        from sklearn.cluster import KMeans\n",
    "        \n",
    "    \n",
    "        \n",
    "        num_columns = data.shape[1]\n",
    "        centroids_list = []\n",
    "        assignment_distances_list = []\n",
    "        \n",
    "        # Iterate over each column\n",
    "        for i in range(num_columns):\n",
    "            # Remove the i-th column from the data\n",
    "            data_subset = np.delete(data, i, axis=1)\n",
    "            \n",
    "            # Perform K-means clustering\n",
    "            kmeans = kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=10).fit(data_subset)\n",
    "            \n",
    "            # Get cluster centroids\n",
    "            centroids = kmeans.cluster_centers_\n",
    "            centroids_list.append(centroids)\n",
    "            \n",
    "            # Assign clusters and distances\n",
    "            cluster_assignment, distances = self.assign_clusters_find_distances(data_subset, centroids)\n",
    "            assignment_distances_list.append(cluster_assignment)\n",
    "            assignment_distances_list.append(distances)\n",
    "        \n",
    "        # Stack centroids into a single array\n",
    "        stacked_centroids = np.stack(centroids_list, axis=1)\n",
    "    \n",
    "        # Stack assignments and distances into a single array\n",
    "        stacked_assignment_distances = np.column_stack(assignment_distances_list)\n",
    "        \n",
    "        return stacked_centroids, stacked_assignment_distances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def kmeans_clustering_with_distances_new_data(self, stacked_centroids, data): \n",
    "            num_columns = stacked_centroids.shape[1]\n",
    "            assignment_distances_list = []\n",
    "            \n",
    "            # Iterate over each column\n",
    "            for i in range(num_columns):\n",
    "                # Get the centroids for the current iteration\n",
    "                centroids = stacked_centroids[:, i, :]\n",
    "                \n",
    "                # Remove the i-th column from the data\n",
    "                data_subset = np.delete(data, i, axis=1)\n",
    "                \n",
    "                # Assign clusters and distances using the provided centroids\n",
    "                cluster_assignment, distances = self.assign_clusters_find_distances(data_subset, centroids)\n",
    "                assignment_distances_list.append(cluster_assignment)\n",
    "                assignment_distances_list.append(distances)\n",
    "            \n",
    "            # Stack assignments and distances into a single array\n",
    "            assignment_distances = np.column_stack(assignment_distances_list)\n",
    "            \n",
    "            return assignment_distances\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297cb371-695f-4a1f-b560-6b93936de225",
   "metadata": {},
   "source": [
    "## Data Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eac4b7-a148-4106-b429-728b4d2ad030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_analysis:\n",
    "\n",
    "    def calculate_gradient(self, x, y):\n",
    "        if len(x) != len(y):\n",
    "            raise ValueError(\"Arrays must be of the same length\")\n",
    "        \n",
    "        # Perform linear regression to find the slope (gradient)\n",
    "        m, _ = np.polyfit(x, y, 1)\n",
    "        \n",
    "        return m\n",
    "\n",
    "    def calculate_average_grad(self, data_set1, data_set2):\n",
    "        gradients = []\n",
    "        for i in range(data_set1.shape[1]):\n",
    "            gradient = self.calculate_gradient(data_set1[:, i], data_set2[:, i])\n",
    "            gradients.append(gradient)\n",
    "        \n",
    "        gradients = np.array(gradients)\n",
    "        average_gradient = np.mean(gradients)\n",
    "        \n",
    "        return gradients, average_gradient\n",
    "\n",
    "\n",
    "    def calculate_r_square(self, array1, array2):\n",
    "            if array1.shape[1] != array2.shape[1]:\n",
    "                raise ValueError(\"Arrays must have the same number of columns\")\n",
    "            \n",
    "            r_squared_values = []\n",
    "            for i in range(array1.shape[1]):\n",
    "                correlation_matrix = np.corrcoef(array1[:, i], array2[:, i])\n",
    "                correlation_coefficient = correlation_matrix[0, 1]\n",
    "                r_squared = correlation_coefficient ** 2\n",
    "                r_squared_values.append(r_squared)\n",
    "        \n",
    "            r_squared_values = np.array(r_squared_values)\n",
    "            average_r_squared = np.mean(r_squared_values)\n",
    "            \n",
    "            return r_squared_values, average_r_squared\n",
    "    \n",
    "    def plot_hexbin_log_frequency(x, y, xlabel=\"\", ylabel=\"\", title=\"\", gridsize=100, cmap='viridis', colorbar=True):\n",
    "        \"\"\"\n",
    "        Function to create a hexbin plot with log-scaled frequency using Matplotlib.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        # Calculate the hexbin frequencies\n",
    "        hb = plt.hexbin(x, y, gridsize=gridsize, cmap=cmap)\n",
    "        counts = hb.get_array()\n",
    "        nonzero_counts = counts[counts != 0]\n",
    "        \n",
    "        # Set the norm to LogNorm with vmin=0\n",
    "        norm = mcolors.LogNorm(vmin=1, vmax=nonzero_counts.max())\n",
    "        hb = plt.hexbin(x, y, gridsize=gridsize, cmap=cmap, norm=norm)\n",
    "        \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        \n",
    "        if colorbar:\n",
    "            plt.colorbar(hb, label='log(count)')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b6cd9f-59a3-450f-8bff-d8493c8f8001",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372688a-40fa-44e3-aee6-8aa764dc5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_raw = np.loadtxt('train4 .csv',delimiter = ',') #import of data as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187c223-ea81-4ac9-916c-777ef3902c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate categorical and continuous\n",
    "\n",
    "categorical_data = np.delete(x_data_raw, 5, axis=1)\n",
    "x6 = x_data_raw[:, 5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53cb6e4-badc-42af-935c-16acd22004e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd96d2-a2a2-463d-950e-baf0a676723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mapped_min_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede472b0-ddd5-41ff-b74d-84a54be1eb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd3c13-8aa7-4c80-b248-a8db6c50d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instances for classes\n",
    "preprocessor = preprocessing()\n",
    "Cluster = clustering()\n",
    "Analyse = data_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e4291-94c5-4632-84ed-4aef11204fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Categorical Data\n",
    "\n",
    "#apply even spacing\n",
    "\n",
    "mapped = preprocessor.map_unique_values_to_columns(categorical_data)\n",
    "\n",
    "#normalise \n",
    "mapped_min_max = preprocessing.min_max(mapped)\n",
    "\n",
    "#cluster\n",
    "centroids, clustered = Cluster.kmean_clustering_with_distances(mapped_min_max, 10)\n",
    "\n",
    "#add min max to cluster number\n",
    "normalised_clustered = preprocessing.min_max_odd_columns(clustered)\n",
    "\n",
    "print(normalised_clustered)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c1dfc-5ba4-4d38-9500-c2ab41a0fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Continuous Data\n",
    "\n",
    "log_x6 = np.log10(x6)\n",
    "x6_logged_normalized = preprocessing.min_max(log_x6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ddfe0e-f4ee-4ca7-8055-71fb77d72188",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_data = np.column_stack((normalised_clustered, x6_logged_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faab48d-df90-4deb-a6f6-6ebb9b2a1a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(clustered, columns=['1', '2', '3','4','5','6','7','8','9','10'])\n",
    "df.to_csv('Clustered input') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3dfa1f-4b7f-40aa-b094-f719fe13abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea886597-97e3-4f91-a25e-bb925ef6b3a6",
   "metadata": {},
   "source": [
    "## Train AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f35c63-4f73-4e1f-8e2f-6d52eba5c386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "AutoEncoder = AutoEncoder(x_train)\n",
    "AutoEncoder.compile(optimizer=keras.optimizers.Adam())\n",
    "AutoEncoder.fit(x_train, epochs= 2000 , batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7de27f-6e69-4334-8683-29523377b51a",
   "metadata": {},
   "source": [
    "## Analyse Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab3aa4-34f8-4988-81af-be7a9d40487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run with Trianing Data\n",
    "\n",
    "latent_space_representation = AutoEncoder.encoder.predict(Input_data)\n",
    "\n",
    "Output_data = AutoEncoder.decoder.predict(latent_space_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb97064-3fb2-44f7-9c09-0073bab09689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Reconstruction Loss\n",
    "Reconstruction_loss = np.mean(np.square(x_train - Output_data), axis = 1)\n",
    "Threshold = np.percentile(Reconstruction_loss, 99.0)\n",
    "print(Threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715924c-6291-4172-a4cf-bea96a88a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Gradients\n",
    "all_gradients, average_gradient = Analyse.calculate_average_grad(Input_data, Output_data)\n",
    "print(average_gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d9c59-9323-4036-90b9-76e8949f0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find r^2 values\n",
    "all_r_squared, average_r_squared = Analyse.calculate_r_square(Input_data, Output_data)\n",
    "print(average_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5423b38-f325-4a9f-9a5f-fc8fb087996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot input vs output\n",
    "data_analysis.plot_hexbin_log_frequency(Input_data[:,10], Output_data[:, 10] , xlabel='Input', ylabel='Output', title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d1f4d-54f4-4871-85e5-cacea335b305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764004f3-24f0-4d2f-b785-974677d40910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1fcdb42-c3a0-4e38-bfb5-88616e25ac06",
   "metadata": {},
   "source": [
    "# Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbf38b-f420-4e55-b718-898501a89ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import test data as numpy array \n",
    "Testing_data = np.loadtxt('testing_data2.csv',delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb3d33-bf4b-4534-8545-4efe0dcc8a0b",
   "metadata": {},
   "source": [
    "## Preprocess Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2518da1-a246-4122-9eb9-a5952936124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate categorical and continuous\n",
    "\n",
    "categorical_data_test = np.delete(Testing_data, 5, axis=1)\n",
    "x6_test = Testing_data[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d00d0a-4bba-4cb1-bf8e-b0d92c2d87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Categorical Data\n",
    "\n",
    "#apply even spacing\n",
    "mapped_test = preprocessor.map_unique_values_to_columns(categorical_data_test)\n",
    "\n",
    "#normalise \n",
    "mapped_min_max_test = preprocessing.min_max(mapped_test)\n",
    "\n",
    "#cluster with same centroids used for training data \n",
    "clustered_test = Cluster.kmeans_clustering_with_distances_new_data(centroids, mapped_min_max_test) \n",
    "\n",
    "#add min max to cluster number\n",
    "normalised_clustered_test = preprocessing.min_max_odd_columns(clustered_test)\n",
    "\n",
    "print(normalised_clustered_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f90ff-ec7f-46dc-a4e9-c4ab4910a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess continuous data\n",
    "\n",
    "log_x6_test = np.log10(x6_test)\n",
    "x6_logged_normalized_test = preprocessing.min_max(log_x6_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c133a-0bd5-4947-9715-880ddd2ad24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_data_test = np.column_stack((normalised_clustered_test, x6_logged_normalized_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c7e05-f48e-482e-9e51-20ce8501e06f",
   "metadata": {},
   "source": [
    "## AutoEncoder on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08ef28-4932-45b1-976d-7ec2699e0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_representation_test = AutoEncoder.encoder.predict(Input_data_test)\n",
    "\n",
    "Output_data_test = AutoEncoder.decoder.predict(latent_space_representation_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678aa90-d007-4065-9f45-0f158ecaab20",
   "metadata": {},
   "source": [
    "## Find Anomalies using Training Data Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b267f-0fe4-4390-a7ba-b473095a2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reconstruction_loss = np.mean(np.square(Input_data_test - Output_data_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb19006-9b22-4843-9256-36781e903ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_truth = Reconstruction_loss > Threshold\n",
    "\n",
    "anomaly_indices = np.where(anomaly_truth)[0]\n",
    "\n",
    "anomalies = Input_data_test[anomaly_truth]\n",
    "\n",
    "print(\"Anomalies:\")\n",
    "print(anomalies)\n",
    "\n",
    "print(\"Indices of anomalies in Test_data:\")\n",
    "print(anomaly_indices)\n",
    "\n",
    "print(\"Number of anomalies:\", len(anomalies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c1178a-410f-439a-af27-2a05f82f0189",
   "metadata": {},
   "source": [
    "## Test Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075923c6-b6e4-42e4-a710-f91e0ca7353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_grad, Test_grad_average = Analyse.calculate_average_grad(Input_data_test, Output_data_test)\n",
    "print(Test_grad_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2a2e7-e291-45a6-800a-cb021d33ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_R2, Test_R2_average = Analyse.calculate_r_square(Input_data_test, Output_data_test)\n",
    "print(Test_R2_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc0535-8c73-4663-bcde-04c3cd8e94f2",
   "metadata": {},
   "source": [
    "## Compare to George IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd293838-1029-4287-966f-bf12c48c03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('George_Anomalies.csv')\n",
    "George_IDs = df['id']\n",
    "array_of_values = np.array(George_IDs, dtype=int)\n",
    "print(array_of_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e7ec7-ea8d-47ba-b0f1-f18ef12c6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the intersection of the two arrays\n",
    "intersection = np.intersect1d(George_IDs, anomaly_indices)\n",
    "\n",
    "# Get the number of values shared\n",
    "shared_count = len(intersection)\n",
    "\n",
    "print(\"Number of shared values:\", shared_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
